{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13040530,"sourceType":"datasetVersion","datasetId":8257582},{"sourceId":13067516,"sourceType":"datasetVersion","datasetId":8270275,"isSourceIdPinned":true},{"sourceId":13125357,"sourceType":"datasetVersion","datasetId":8314638}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset & Training Setup\n\nWe combine **GTSDB (German Traffic Sign Detection Benchmark)** and **GTSRB (German Traffic Sign Recognition Benchmark)** to build a larger and more balanced training dataset.\n\n## Original Data (GTSDB)\n- ~900 images available:\n  - **600** used for training\n  - **300** for validation\n- Validation set contains **only original images** (no synthetic augmentation).\n\n## Augmented Data\n- To avoid underfitting due to limited original samples, **~13k augmented images** were generated using GTSRB and GTSDB signs placed on empty backgrounds with various transformations.\n- Total training data: **~14,958 images**  \n  (â‰ˆ13,158 augmented + 600 originals are upsampled Ã—2 treated as 1,800 samples)\n\n## Ratio-based Training\n- Training uses a **DynamicMixedDataset** that controls the ratio of original vs augmented images per epoch:\n  - Early epochs â†’ more augmented images (for diversity)  \n  - Later epochs â†’ more original images (for realism)\n\n## Validation\n- Validation strictly uses the **300 original images**.  \n- This ensures evaluation is fair and not biased by synthetic data.\n\n## Dataset Structure\nimages/\n- train_org/ # 1800 (600 originals are upsampled Ã—2)\n- train_aug/ # ~13k augmented\n- val/ # 300 original (validation only)\n\n\n## Why this setup?\n1. Large and diverse training dataset (~15k images).  \n2. Balanced use of augmented vs real images with a dynamic ratio schedule.  \n3. Reliable validation on untouched original data only.  \n\n---\n\n## Download\nThe dataset is available on Kaggle:  \nðŸ‘‰ [German Traffic Signs Detection (YOLO, Aug + Org)](https://www.kaggle.com/datasets/wahburrehman/german-traffic-signs-detection-yolo-aug-org)\n","metadata":{}},{"cell_type":"code","source":"!pip -q install ultralytics opencv-python\n\n# verify GPU\n!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============== CUSTOM DATASET STRUCTURE ===============","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile custom_dataset.py\nimport os\nimport random\nimport tempfile\nimport yaml\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\nfrom ultralytics.data.dataset import YOLODataset\n\nclass DynamicMixedDataset(Dataset):\n    \"\"\"\n    A custom dataset that dynamically mixes original and augmented images each epoch\n    based on a provided ratio schedule, all within a continuous training loop.\n    \"\"\"\n    def __init__(\n        self,\n        original_img_dir,\n        augmented_img_dir,\n        data_yaml_path,\n        img_size=640,\n        augment=True,\n        hyp=None,\n        ratio_schedule_fn=None,\n        total_target=9000,\n    ):\n        \"\"\"\n        Args:\n            original_img_dir: Path to directory with original training images\n            augmented_img_dir: Path to directory with augmented training images\n            data_yaml_path: Path to the original data.yaml file\n            ratio_schedule_fn: Function(epoch) -> (original_ratio, augmented_ratio)\n            total_target: Total number of images to use per epoch\n        \"\"\"\n        self.original_img_dir = Path(original_img_dir)\n        self.augmented_img_dir = Path(augmented_img_dir)\n        self.data_yaml_path = data_yaml_path\n        self.img_size = img_size\n        self.augment = augment\n        self.hyp = hyp\n        self.ratio_schedule_fn = ratio_schedule_fn\n        self.total_target = total_target\n        self.current_epoch = 0\n\n        # Load base data config (for val path and classes)\n        with open(data_yaml_path, \"r\") as f:\n            self.base_data_config = yaml.safe_load(f)\n\n        # Get all image paths\n        self.original_images = self._get_image_paths(self.original_img_dir)\n        self.augmented_images = self._get_image_paths(self.augmented_img_dir)\n\n        print(f\"Found {len(self.original_images)} original images\")\n        print(f\"Found {len(self.augmented_images)} augmented images\")\n\n        # Initialize the dataset\n        self.base_dataset = None\n        self.current_epoch_config = None\n        self.temp_train_file = None\n        self.refresh_epoch_mix()\n\n    def _get_image_paths(self, directory: Path):\n        \"\"\"Get all image paths from a directory recursively.\"\"\"\n        extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n        return sorted(\n            [p.as_posix() for p in directory.rglob(\"*\") if p.suffix.lower() in extensions]\n        )\n\n    def refresh_epoch_mix(self):\n        \"\"\"Create a new mix of images for the current epoch.\"\"\"\n        # Get ratios for current epoch\n        if self.ratio_schedule_fn:\n            org_ratio, aug_ratio = self.ratio_schedule_fn(self.current_epoch)\n        else:\n            org_ratio, aug_ratio = 0.6, 0.4  # Default\n\n        # Calculate how many of each to use\n        org_target = int(org_ratio * self.total_target)\n        aug_target = int(aug_ratio * self.total_target)\n\n        # Helper: sample with replacement if needed\n        def sample_with_replacement(pool, target):\n            if target <= 0:\n                return []\n            if len(pool) >= target:\n                return random.sample(pool, target)\n            reps, rem = divmod(target, len(pool))\n            selected = pool * reps + random.sample(pool, rem)\n            random.shuffle(selected)\n            return selected\n\n        selected_org = sample_with_replacement(self.original_images, org_target)\n        selected_aug = sample_with_replacement(self.augmented_images, aug_target)\n\n        # Combine and shuffle\n        epoch_images = selected_org + selected_aug\n        random.shuffle(epoch_images)\n\n        print(\n            f\"Epoch {self.current_epoch}: Using {len(selected_org)} original + \"\n            f\"{len(selected_aug)} augmented images (Ratio: {org_ratio:.1f}/{aug_ratio:.1f})\"\n        )\n\n        # Create/replace temporary train.txt file\n        # Clean old temp (if any)\n        if self.temp_train_file is not None:\n            try:\n                os.unlink(self.temp_train_file.name)\n            except Exception:\n                pass\n\n        self.temp_train_file = tempfile.NamedTemporaryFile(\n            mode=\"w\", suffix=\".txt\", delete=False\n        )\n        self.temp_train_file.write(\"\\n\".join(epoch_images))\n        self.temp_train_file.close()\n\n        # Build a per-epoch data config that points 'train' to temp file\n        epoch_config = {\n            \"path\": self.base_data_config.get(\"path\", \"\"),  # optional\n            \"train\": self.temp_train_file.name,             # per-epoch list\n            \"val\": self.base_data_config[\"val\"],\n            \"names\": self.base_data_config[\"names\"],\n            \"nc\": self.base_data_config[\"nc\"],\n        }\n        self.current_epoch_config = epoch_config\n\n        # after you build epoch_config dict\n        self.temp_yaml_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False)\n        yaml.safe_dump(epoch_config, self.temp_yaml_file)\n        self.temp_yaml_file.close()\n\n        self.current_epoch_yaml = self.temp_yaml_file.name  # path to the temp YAML file\n\n        # Build the internal YOLO dataset\n        self.base_dataset = YOLODataset(\n            img_path=epoch_config[\"train\"],  # required by BaseDataset\n            data=epoch_config,\n            task=\"detect\",\n            imgsz=self.img_size,\n            augment=self.augment,\n        )\n\n    def set_epoch(self, epoch: int):\n        \"\"\"Call this at the start of each epoch to refresh the mix.\"\"\"\n        self.current_epoch = int(epoch)\n        self.refresh_epoch_mix()\n\n    def __len__(self):\n        return len(self.base_dataset)\n\n    def __getitem__(self, index):\n        return self.base_dataset[index]\n\n    def __del__(self):\n        # Cleanup temporary file\n        if getattr(self, \"temp_train_file\", None) is not None:\n            try:\n                os.unlink(self.temp_train_file.name)\n            except Exception:\n                pass\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Model Training  ===\nfrom ultralytics import YOLO\nfrom ultralytics.data import build_yolo_dataset, build_dataloader\nfrom custom_dataset import DynamicMixedDataset\n\n# ratio schedule\ndef ratio_schedule(epoch: int):\n    if epoch <= 40:\n        return (0.4, 0.6)   # 40% original, 60% augmented\n    elif epoch <= 80:\n        return (0.6, 0.4)   # 60% original, 40% augmented\n    elif epoch <= 90:\n        return (0.8, 0.2)   # 80% original, 20% augmented\n    else:\n        return (1.0, 0.0)   # 100% original, 0% augmented\n\n# paths (adjust for your environment)\nbase_path = \"/kaggle/input/aug-org-traffic-sign-detection-yolo-format\"\noriginal_dir = f\"{base_path}/images/train_org\"\naugmented_dir = f\"{base_path}/images/train_aug\"\ndata_yaml_path = f\"{base_path}/data.yaml\"\n\n# build dynamic dataset (creates Epoch 0 mix)\ntrain_dataset = DynamicMixedDataset(\n    original_img_dir=original_dir,\n    augmented_img_dir=augmented_dir,\n    data_yaml_path=data_yaml_path,\n    img_size=1024,\n    augment=True,\n    ratio_schedule_fn=ratio_schedule,\n    total_target=9000,\n)\n\n# define a callback to refresh mix each epoch and rebuild the loader\ndef on_train_epoch_start(trainer):\n    # 1) advance schedule and regenerate temp train.txt + YAML\n    train_dataset.set_epoch(trainer.epoch)\n\n    # 2) point trainer to new YAML path so internal checks use the fresh config\n    trainer.args.data = train_dataset.current_epoch_yaml\n\n    # 3) rebuild the train dataloader for this epoch\n    ds = build_yolo_dataset(\n        trainer.args,                                  \n        img_path=train_dataset.current_epoch_config[\"train\"],  # temp .txt\n        batch=trainer.batch_size,\n        data=trainer.data,\n        mode=\"train\",\n    )\n    trainer.train_loader = build_dataloader(\n        ds,                      # dataset\n        trainer.batch_size,      # batch (positional)\n        trainer.args.workers,    # workers\n        True,                    # shuffle\n    )\n\n# init model and register callback\nmodel = YOLO(\"yolov8m.pt\")\nmodel.add_callback(\"on_train_epoch_start\", on_train_epoch_start)\n\n# train starting from the current mixed config (epoch 0 list)\nresults = model.train(\n    data=train_dataset.current_epoch_yaml,\n    epochs=100,\n    imgsz=1024,\n    batch=8,\n    device=0,\n    workers=2,\n    cos_lr=True,\n    patience=30,\n    cache=\"ram\",\n    save_period=5, \n    verbose=True,\n    name=\"detect/train\",\n    exist_ok=True,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import inspect\nprint(inspect.getsource(build_yolo_dataset))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}